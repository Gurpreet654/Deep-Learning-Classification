{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpRDtOuB4FyF1sPEdR30kp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gurpreet654/Machine-Learning-Classification/blob/main/Classification_Deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries**"
      ],
      "metadata": {
        "id": "6xnE7B-rvP1W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "MHsZj0wwCmFh",
        "outputId": "c1c1eab3-e39c-4e5f-9289-9546bb488983"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c012e77fb0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Define data path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D:\\MIDD\\data_MIDD'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdata_dir_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\MIDD\\\\data_MIDD'"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "Created on Thu May  4 23:32:34 2017\n",
        "@author: anuj shah\n",
        "\"\"\"\n",
        "# Import libraries\n",
        "import os,cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "#K.set_image_dim_ordering('th')\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "#from keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Define data path\n",
        "data_path = 'D:\\MIDD\\data_MIDD'\n",
        "data_dir_list = os.listdir(data_path)\n",
        "\n",
        "img_rows=128\n",
        "img_cols=128\n",
        "num_channel=1\n",
        "num_epoch=5\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = 4\n",
        "\n",
        "labels_name={'RGB1':0,'T1':1,'GT1':2}\n",
        "\n",
        "img_data_list=[]\n",
        "labels_list = []\n",
        "\n",
        "for dataset in data_dir_list:\n",
        "\timg_list=os.listdir(data_path+'/'+ dataset)\n",
        "\tprint ('Loading the images of dataset-'+'{}\\n'.format(dataset))\n",
        "\tlabel = labels_name[dataset]\n",
        "\tfor img in img_list:\n",
        "\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
        "\t\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
        "\t\tinput_img_resize=cv2.resize(input_img,(128,128))\n",
        "\t\timg_data_list.append(input_img_resize)\n",
        "\t\tlabels_list.append(label)\n",
        "\n",
        "img_data = np.array(img_data_list)\n",
        "img_data = img_data.astype('float32')\n",
        "img_data /= 255\n",
        "print (img_data.shape)\n",
        "\n",
        "labels = np.array(labels_list)\n",
        "# print the count of number of samples for different classes\n",
        "print(np.unique(labels,return_counts=True))\n",
        "# convert class labels to on-hot encoding\n",
        "import tensorflow as tf\n",
        "Y = tf.keras.utils.to_categorical(labels, num_classes)\n",
        "\n",
        "#Shuffle the dataset\n",
        "x,y = shuffle(img_data,Y, random_state=2)\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
        "#\n",
        "'''if num_channel==1:\n",
        "\tif K.image_dim_ordering=='th':\n",
        "\t\timg_data= np.expand_dims(img_data, axis=1) \n",
        "\t\tprint (img_data.shape)\n",
        "\telse:\n",
        "\t\timg_data= np.expand_dims(img_data, axis=(len(img_data.shape)) )\n",
        "\t\tprint(img_data.shape)\n",
        "        \n",
        "else:\n",
        "\tif K.backend.image_dim_ordering()=='th':\n",
        "\t\timg_data=np.rollaxis(img_data,3,1)\n",
        "\t\tprint (img_data.shape)'''\n",
        "img_data= np.expand_dims(img_data, axis=3) \n",
        "\n",
        "#%%\n",
        "USE_SKLEARN_PREPROCESSING=False\n",
        "\n",
        "if USE_SKLEARN_PREPROCESSING:\n",
        "\t# using sklearn for preprocessing\n",
        "\tfrom sklearn import preprocessing\n",
        "\t\n",
        "\tdef image_to_feature_vector(image, size=(128, 128)):\n",
        "\t\t# resize the image to a fixed size, then flatten the image into\n",
        "\t\t# a list of raw pixel intensities\n",
        "\t\treturn cv2.resize(image, size).flatten()\n",
        "\t\n",
        "\timg_data_list=[]\n",
        "\tfor dataset in data_dir_list:\n",
        "\t\timg_list=os.listdir(data_path+'/'+ dataset)\n",
        "\t\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
        "\t\tfor img in img_list:\n",
        "\t\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
        "\t\t\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
        "\t\t\tinput_img_flatten=image_to_feature_vector(input_img,(128,128))\n",
        "\t\t\timg_data_list.append(input_img_flatten)\n",
        "\t\n",
        "\timg_data = np.array(img_data_list)\n",
        "\timg_data = img_data.astype('float32')\n",
        "\tprint (img_data.shape)\n",
        "\timg_data_scaled = preprocessing.scale(img_data)\n",
        "\tprint (img_data_scaled.shape)\n",
        "\t\n",
        "\tprint (np.mean(img_data_scaled))\n",
        "\tprint (np.std(img_data_scaled))\n",
        "\t\n",
        "\tprint (img_data_scaled.mean(axis=0))\n",
        "\tprint (img_data_scaled.std(axis=0))\n",
        "\t\n",
        "\tif tf.keras.backend.image_data_format()=='th':\n",
        "\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
        "\t\tprint (img_data_scaled.shape)\n",
        "\t\t\n",
        "\telse:\n",
        "\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
        "\t\tprint (img_data_scaled.shape)\n",
        "\t\n",
        "\t\n",
        "\tif K.image_dim_ordering()=='th':\n",
        "\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
        "\t\tprint (img_data_scaled.shape)\n",
        "\t\t\n",
        "\telse:\n",
        "\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
        "\t\tprint (img_data_scaled.shape)\n",
        "\n",
        "if USE_SKLEARN_PREPROCESSING:\n",
        "\timg_data=img_data_scaled\n",
        "\n",
        "#%%\n",
        "# Defining the model\n",
        "num_classes=4\n",
        "'''input_shape=img_data[0].shape\n",
        "#imput\t\t\t\t\t\n",
        "model = Sequential()\n",
        "\n",
        "#model.add(Convolution2D(32,(3,3),input_shape=input_shape))\n",
        "model.add(Convolution2D(32, (3, 3),padding='same', input_shape = (128, 128, 1)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Convolution2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Convolution2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Convolution2D(64, 3, 3))\n",
        "#model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=SGD,metrics=[\"accuracy\"])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd',metrics=[\"accuracy\"])\n",
        "#model.compile(optimizer='sgd', loss='mse')'''\n",
        "# Viewing model_configuration\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Conv2D\n",
        "model=Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32,kernel_size=3,padding=\"same\",activation=\"relu\",input_shape=(128,128,1)))\n",
        "#model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Conv2D(filters=32,kernel_size=3,activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(filters=64,kernel_size=3,activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "'''model.add(Convolution2D(32, (3, 3),padding='same', input_shape = (128, 128, 1)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Convolution2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))'''\n",
        "\n",
        "model.add(Dense(64,activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes,activation=\"softmax\"))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.get_config()\n",
        "model.layers[0].get_config()\n",
        "model.layers[0].input_shape\t\t\t\n",
        "model.layers[0].output_shape\t\t\t\n",
        "model.layers[0].get_weights()\n",
        "np.shape(model.layers[0].get_weights()[0])\n",
        "model.layers[0].trainable\n",
        "\n",
        "#%%\n",
        "#training_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "X_train1=np.expand_dims(X_train, -1)\n",
        "X_test1=np.expand_dims(X_test, -1)\n",
        "y_train1=np.expand_dims(y_train, -1)\n",
        "y_test1=np.expand_dims(y_test, -1)\n",
        "training_set=(X_train1, y_train1)\n",
        "#test_set=(X_test1, y_test1)\n",
        "#model.fit_generator(training_set,validation_data=test_set,epochs=10, steps_per_epoch=len(training_set), validation_steps=len(test_set) )\n",
        "\n",
        "\n",
        "#%%\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "import tensorflow as tf\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "X_TRAIN = np.array(X_train.reshape(-1, 128, 128,1))\n",
        "X_TEST = np.array(X_test.reshape(-1, 128, 128,1))\n",
        "\n",
        "#model.fit(X_TRAIN,y_train, epochs=10, validation_split=0.1)\n",
        "\n",
        "#%%\n",
        "\n",
        "hist = model.fit(X_TRAIN, y_train, batch_size=16, epochs=num_epoch, verbose=1, validation_data=(X_TEST, y_test))\n",
        "#%%\n",
        "pred_y=model.predict(X_TEST)\n",
        "#%%\n",
        "#images/humans/rider-200.jpg\n",
        "aa1=cv2.imread('GURJINDER.jpg')\n",
        "aa4=cv2.cvtColor(aa1, cv2.COLOR_BGR2RGB)\n",
        "aa2=cv2.cvtColor(aa1, cv2.COLOR_BGR2GRAY)\n",
        "aa2=cv2.resize(aa2,(128,128))\n",
        "test_img = np.array(aa2.reshape(-1, 128, 128,1))\n",
        "pp=model.predict(test_img)\n",
        "pp1 = np.argmax(pp, axis=1)\n",
        "#predict_number=16\n",
        "#import matplotlib.pyplot as plt\n",
        "plt.imshow(aa4)\n",
        "labels_name=['Cat','Dog','Horse','Human']\n",
        "plt.title(labels_name[pp1[0]])\n",
        "\n",
        "#%%\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import itertools\n",
        "\n",
        "Y_pred = model.predict(X_TEST)\n",
        "print(Y_pred)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print(y_pred)\n",
        "#y_pred = model.predict_classes(X_test)\n",
        "#print(y_pred)\n",
        "target_names = ['class 0(cats)', 'class 1(Dogs)', 'class 2(Horses)','class 3(Humans)']\n",
        "\t\t\t\t\t\n",
        "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
        "\n",
        "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
        "\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
        "                      title='Confusion matrix')\n",
        "#plt.figure()\n",
        "# Plot normalized confusion matrix\n",
        "#plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,\n",
        "#                      title='Normalized confusion matrix')\n",
        "#plt.figure()\n",
        "plt.show()\n",
        "\n",
        "#%%\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "model.save('model.hdf5')\n",
        "loaded_model=load_model('model.hdf5')\n",
        "#%%\n",
        "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
        "activation_model = tf.keras.Model(model.input,layer_outputs)\n",
        "activations = activation_model.predict(test_img)\n",
        "  \n",
        "# Getting Activations of first layer\n",
        "first_layer_activation = activations[1]\n",
        "  \n",
        "# shape of first layer activation\n",
        "print(first_layer_activation.shape)\n",
        "  \n",
        "# 6th channel of the image after first layer of convolution is applied\n",
        "plt.matshow(first_layer_activation[0, :, :, 8], cmap ='viridis')\n",
        "  \n",
        "# 15th channel of the image after first layer of convolution is applied\n",
        "plt.matshow(first_layer_activation[0, :, :, 1], cmap ='viridis')\n",
        "\n",
        "\n"
      ]
    }
  ]
}